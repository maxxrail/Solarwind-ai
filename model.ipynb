{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Necessary Libraries ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ONCE DETECTRON2 IS INSTALLED, PLEASE COPY THE CONTENTS FROM THE SUBFOLDER DETECTRON2 FOLDER TO THE MAIN DETECTRON2 FOLDER!!!\n",
    "\n",
    "# Upgrade pip\n",
    "%pip install --upgrade pip\n",
    "\n",
    "# Install OpenCV for image processing\n",
    "%pip install opencv-python\n",
    "\n",
    "# Install numpy at the desired version\n",
    "%pip install numpy==1.26.1\n",
    "\n",
    "# Install PyTorch and related packages (adjust versions/URLs as needed for your platform)\n",
    "%pip install torch==2.0.0+cu117 torchvision==0.15.1+cu117 torchaudio==2.0.0+cu117 --index-url https://download.pytorch.org/whl/cu117\n",
    "\n",
    "# Install gitpython and cython if not already installed\n",
    "%pip install gitpython cython\n",
    "\n",
    "# Install dependencies, but pin fvcore and pycocotools to compatible versions:\n",
    "%pip install cython\n",
    "\n",
    "# Install fvcore.\n",
    "%pip install fvcore==0.1.5.post20221221\n",
    "\n",
    "# Install pycocotools\n",
    "%pip install pycocotools==2.0.8\n",
    "\n",
    "# ---- Additional installations ----\n",
    "%pip install docutils==0.19\n",
    "%pip install sphinx==7\n",
    "%pip install recommonmark==0.6.0\n",
    "%pip install sphinx_rtd_theme\n",
    "%pip install termcolor\n",
    "%pip install yacs\n",
    "%pip install tabulate\n",
    "%pip install cloudpickle\n",
    "%pip install future\n",
    "# (The following two lines with CPU wheels are for Linux CP37; comment them out if not needed)\n",
    "# %pip install https://download.pytorch.org/whl/cpu/torch-1.8.1%2Bcpu-cp37-cp37m-linux_x86_64.whl\n",
    "# %pip install https://download.pytorch.org/whl/cpu/torchvision-0.9.1%2Bcpu-cp37-cp37m-linux_x86_64.whl\n",
    "%pip install \"omegaconf>=2.1.0.dev24\"\n",
    "%pip install \"hydra-core>=1.1.0.dev5\"\n",
    "%pip install scipy\n",
    "%pip install timm\n",
    "# ---------------------------------\n",
    "\n",
    "# Clone the Detectron2 repository and install it in editable mode\n",
    "import os\n",
    "if not os.path.exists('detectron2'):\n",
    "    !git clone https://github.com/facebookresearch/detectron2.git\n",
    "\n",
    "%cd detectron2\n",
    "%pip install -e .\n",
    "%cd ..\n",
    "\n",
    "# Install TensorFlow and Keras for deep learning models\n",
    "%pip install tensorflow\n",
    "\n",
    "# Install pickleshare to fix the IPython warning\n",
    "%pip install pickleshare\n",
    "\n",
    "# Install additional required libraries\n",
    "%pip install numpy matplotlib pandas scikit-learn tqdm Pillow\n",
    "\n",
    "# Install geospatial libraries\n",
    "%pip install geopandas rasterio shapely\n",
    "\n",
    "# Install any other utility libraries if needed\n",
    "%pip install jsonschema pyyaml\n",
    "\n",
    "# Install IPython kernel to ensure compatibility\n",
    "%pip install ipykernel\n",
    "\n",
    "# After installations, you may need to restart the kernel\n",
    "print(\"All libraries installed. Please restart the kernel to ensure all packages are loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from yacs.config import CfgNode as CN\n",
    "import detectron2.data.transforms as T\n",
    "from detectron2.data import detection_utils as utils\n",
    "from detectron2.engine import DefaultTrainer, DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "print(np.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding Dataset (Do not run unless you need to make dataset bigger, the changes are permanent) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the train folder and its subfolders\n",
    "# train_folder = r\"C:\\Users\\itagl\\Downloads\\AerialImageDataset\\train\"\n",
    "# images_folder = os.path.join(train_folder, \"images\")\n",
    "# gt_folder = os.path.join(train_folder, \"gt\")\n",
    "\n",
    "# # Get a list of all TIF images in the images folder\n",
    "# image_paths = glob.glob(os.path.join(images_folder, \"*.tif\"))\n",
    "# print(f\"Found {len(image_paths)} images in {images_folder}.\")\n",
    "\n",
    "# def rotate_image(img, angle):\n",
    "#     \"\"\"Rotate image by given angle (90, 180, 270 degrees).\"\"\"\n",
    "#     if angle == 90:\n",
    "#         return cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "#     elif angle == 180:\n",
    "#         return cv2.rotate(img, cv2.ROTATE_180)\n",
    "#     elif angle == 270:\n",
    "#         return cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "#     else:\n",
    "#         raise ValueError(\"Unsupported rotation angle: \" + str(angle))\n",
    "\n",
    "# for img_path in image_paths:\n",
    "#     base = os.path.splitext(os.path.basename(img_path))[0]  # e.g. 'austin1'\n",
    "#     ext = os.path.splitext(img_path)[1]  # e.g. '.tif'\n",
    "#     # Assume corresponding mask has the same base name in the gt folder\n",
    "#     mask_path = os.path.join(gt_folder, base + ext)\n",
    "    \n",
    "#     # Read the original image and mask\n",
    "#     img = cv2.imread(img_path)\n",
    "#     mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)  # keep mask as-is\n",
    "#     if img is None or mask is None:\n",
    "#         print(f\"Skipping {base}{ext}: cannot load image or mask.\")\n",
    "#         continue\n",
    "    \n",
    "#     # 1. Rotate 90\n",
    "#     img_r90 = rotate_image(img, 90)\n",
    "#     mask_r90 = rotate_image(mask, 90)\n",
    "    \n",
    "#     # 2. Rotate 180\n",
    "#     img_r180 = rotate_image(img, 180)\n",
    "#     mask_r180 = rotate_image(mask, 180)\n",
    "    \n",
    "#     # 3. Rotate 270\n",
    "#     img_r270 = rotate_image(img, 270)\n",
    "#     mask_r270 = rotate_image(mask, 270)\n",
    "    \n",
    "#     # 4. Mirror (flip horizontally)\n",
    "#     img_m = cv2.flip(img, 1)\n",
    "#     mask_m = cv2.flip(mask, 1)\n",
    "    \n",
    "#     # 5. Rotate 90 then mirror\n",
    "#     img_r90m = cv2.flip(img_r90, 1)\n",
    "#     mask_r90m = cv2.flip(mask_r90, 1)\n",
    "    \n",
    "#     # 6. Rotate 180 then mirror\n",
    "#     img_r180m = cv2.flip(img_r180, 1)\n",
    "#     mask_r180m = cv2.flip(mask_r180, 1)\n",
    "    \n",
    "#     # 7. Rotate 270 then mirror\n",
    "#     img_r270m = cv2.flip(img_r270, 1)\n",
    "#     mask_r270m = cv2.flip(mask_r270, 1)\n",
    "    \n",
    "#     # Save new images in the images folder\n",
    "#     cv2.imwrite(os.path.join(images_folder, f\"{base}r90{ext}\"), img_r90)\n",
    "#     cv2.imwrite(os.path.join(images_folder, f\"{base}r180{ext}\"), img_r180)\n",
    "#     cv2.imwrite(os.path.join(images_folder, f\"{base}r270{ext}\"), img_r270)\n",
    "#     cv2.imwrite(os.path.join(images_folder, f\"{base}m{ext}\"), img_m)\n",
    "#     cv2.imwrite(os.path.join(images_folder, f\"{base}r90m{ext}\"), img_r90m)\n",
    "#     cv2.imwrite(os.path.join(images_folder, f\"{base}r180m{ext}\"), img_r180m)\n",
    "#     cv2.imwrite(os.path.join(images_folder, f\"{base}r270m{ext}\"), img_r270m)\n",
    "    \n",
    "#     # Save new masks in the gt folder\n",
    "#     cv2.imwrite(os.path.join(gt_folder, f\"{base}r90{ext}\"), mask_r90)\n",
    "#     cv2.imwrite(os.path.join(gt_folder, f\"{base}r180{ext}\"), mask_r180)\n",
    "#     cv2.imwrite(os.path.join(gt_folder, f\"{base}r270{ext}\"), mask_r270)\n",
    "#     cv2.imwrite(os.path.join(gt_folder, f\"{base}m{ext}\"), mask_m)\n",
    "#     cv2.imwrite(os.path.join(gt_folder, f\"{base}r90m{ext}\"), mask_r90m)\n",
    "#     cv2.imwrite(os.path.join(gt_folder, f\"{base}r180m{ext}\"), mask_r180m)\n",
    "#     cv2.imwrite(os.path.join(gt_folder, f\"{base}r270m{ext}\"), mask_r270m)\n",
    "    \n",
    "#     print(f\"Created 7 copies for {base}{ext}\")\n",
    "\n",
    "# print(\"Done creating rotated and mirrored copies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement get_inria_dicts() ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inria_dicts_split(img_dir, mask_dir, split=\"train\", train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Returns a list of dataset dictionaries for Detectron2.\n",
    "    Splits the dataset into a training set (80% by default)\n",
    "    and a validation set (20% by default) based on the 'split' parameter.\n",
    "    \n",
    "    Assumes each image in img_dir has a corresponding mask in mask_dir\n",
    "    with the same filename.\n",
    "    \n",
    "    Parameters:\n",
    "      - img_dir: Directory containing the images.\n",
    "      - mask_dir: Directory containing the corresponding masks.\n",
    "      - split: \"train\" or \"val\" to specify the subset.\n",
    "      - train_ratio: Proportion of images for training.\n",
    "    \n",
    "    Returns:\n",
    "      - List of dictionaries with image paths and annotations.\n",
    "    \"\"\"\n",
    "    # Get list of image filenames (filter for common image extensions)\n",
    "    image_files = [f for f in os.listdir(img_dir) if f.lower().endswith((\".jpg\", \".png\", \".jpeg\", \".tif\", \".tiff\"))]\n",
    "    \n",
    "    # Shuffle the list for randomness (set seed if reproducibility is needed)\n",
    "    random.shuffle(image_files)\n",
    "    \n",
    "    # Calculate the split index\n",
    "    split_index = int(len(image_files) * train_ratio)\n",
    "    \n",
    "    # Select files based on split\n",
    "    if split == \"train\":\n",
    "        selected_files = image_files[:split_index]\n",
    "    else:  # \"val\"\n",
    "        selected_files = image_files[split_index:]\n",
    "    \n",
    "    dataset_dicts = []\n",
    "    for idx, image_file in enumerate(selected_files):\n",
    "        record = {}\n",
    "        img_path = os.path.join(img_dir, image_file)\n",
    "        base_name = os.path.splitext(image_file)[0]\n",
    "        mask_path = os.path.join(mask_dir, image_file)  # same filename for mask\n",
    "        \n",
    "        # Read the image to get its dimensions\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        height, width = img.shape[:2]\n",
    "        \n",
    "        record[\"file_name\"] = img_path\n",
    "        record[\"image_id\"] = idx\n",
    "        record[\"height\"] = height\n",
    "        record[\"width\"] = width\n",
    "        \n",
    "        # Read the corresponding mask in grayscale\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if mask is None:\n",
    "            continue\n",
    "        \n",
    "        # Threshold the mask to ensure it's binary\n",
    "        _, thresh = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        objs = []\n",
    "        for cnt in contours:\n",
    "            if cv2.contourArea(cnt) < 10:\n",
    "                continue\n",
    "            poly = cnt.flatten().tolist()\n",
    "            if len(poly) < 6:\n",
    "                continue\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            objs.append({\n",
    "                \"bbox\": [x, y, w, h],\n",
    "                \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                \"segmentation\": [poly],\n",
    "                \"category_id\": 0\n",
    "            })\n",
    "        record[\"annotations\"] = objs\n",
    "        dataset_dicts.append(record)\n",
    "    return dataset_dicts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the dataset for Detectron2 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the dataset with Detectron2\n",
    "# Define the paths to your image and mask folders\n",
    "train_img_dir = r\"C:\\Users\\itagl\\Downloads\\AerialImageDataset\\train\\images\"\n",
    "train_mask_dir = r\"C:\\Users\\itagl\\Downloads\\AerialImageDataset\\train\\gt\"\n",
    "\n",
    "# Register training subset (80% of the image-mask pairs)\n",
    "DatasetCatalog.register(\"inria_train\", lambda: get_inria_dicts_split(train_img_dir, train_mask_dir, split=\"train\"))\n",
    "MetadataCatalog.get(\"inria_train\").set(thing_classes=[\"rooftop\"])\n",
    "\n",
    "# Register validation subset (20% of the image-mask pairs)\n",
    "DatasetCatalog.register(\"inria_val\", lambda: get_inria_dicts_split(train_img_dir, train_mask_dir, split=\"val\"))\n",
    "MetadataCatalog.get(\"inria_val\").set(thing_classes=[\"rooftop\"])\n",
    "\n",
    "print(\"Registered 'inria_train' and 'inria_val' datasets.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize sample data to ensure it's loaded correctly ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a few samples from the dataset\n",
    "dataset_dicts = DatasetCatalog.get(\"inria_train\")\n",
    "num_samples = 3\n",
    "\n",
    "for d in random.sample(dataset_dicts, num_samples):\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    if img is None:\n",
    "        continue\n",
    "    visualizer = Visualizer(img[:, :, ::-1], MetadataCatalog.get(\"inria_train\"), scale=1.0)\n",
    "    out = visualizer.draw_dataset_dict(d)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(out.get_image()[:, :, ::-1])\n",
    "    plt.title(\"Sample Data Visualization\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and train the Mask R-CNN model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration\n",
    "cfg = get_cfg()\n",
    "# Load a Mask R-CNN config from the model zoo\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"))\n",
    "\n",
    "# Specify the training and validation datasets\n",
    "cfg.DATASETS.TRAIN = (\"inria_train\",)\n",
    "cfg.DATASETS.TEST = (\"inria_val\",)  # used for evaluation during training\n",
    "\n",
    "# DataLoader and augmentation settings\n",
    "cfg.DATALOADER.NUM_WORKERS = 12   # adjust based on your CPU capabilities\n",
    "\n",
    "# Load pretrained weights\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\")\n",
    "\n",
    "# Solver settings (adjust iterations and learning rate as needed)\n",
    "cfg.SOLVER.IMS_PER_BATCH = 6    # images per batch\n",
    "cfg.SOLVER.BASE_LR = 0.00025    # learning rate\n",
    "cfg.SOLVER.MAX_ITER = 15000      # number of iterations, adjust based on convergence\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256  # number of proposals per image\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only one class: \"rooftop\"\n",
    "\n",
    "# Optional: Enable built-in random cropping to 4000x4000 if desired\n",
    "cfg.INPUT.CROP.ENABLED = True\n",
    "cfg.INPUT.CROP.TYPE = \"absolute\"\n",
    "cfg.INPUT.CROP.SIZE = [2500, 2500]  # crop size (height, width)\n",
    "\n",
    "# Set the output directory for model checkpoints and logs\n",
    "cfg.OUTPUT_DIR = \"./output_inria\"\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize the trainer with the above configuration\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Model on Validation set ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the model on the validation set\n",
    "evaluator = COCOEvaluator(\"inria_val\", cfg, False, output_dir=cfg.OUTPUT_DIR)\n",
    "val_loader = build_detection_test_loader(cfg, \"inria_val\")\n",
    "print(inference_on_dataset(trainer.model, val_loader, evaluator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly select test image ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to you path for test images\n",
    "test_images_dir = r\"C:\\Users\\itagl\\Downloads\\Pics\\Pics\" \n",
    "# test_images_dir = r\"C:\\Users\\itagl\\Downloads\\AerialImageDataset\\test\\images\"\n",
    "\n",
    "\n",
    "# Valid image extensions\n",
    "img_extensions = [\".jpg\", \".jpeg\", \".png\", \".tif\"]\n",
    "all_test_files = [\n",
    "    f for f in os.listdir(test_images_dir)\n",
    "    if os.path.splitext(f)[1].lower() in img_extensions\n",
    "]\n",
    "\n",
    "if not all_test_files:\n",
    "    raise ValueError(\"No valid images found in test_images_dir!\")\n",
    "\n",
    "# Randomly pick one\n",
    "chosen_file = random.choice(all_test_files)\n",
    "test_image_path = os.path.join(test_images_dir, chosen_file)\n",
    "\n",
    "print(\"Randomly chosen test image:\", test_image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform inference using the trained model and visualize ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the configuration to use the final model weights\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.4   # Set the confidence threshold\n",
    "cfg.TEST.DETECTIONS_PER_IMAGE = 1000\n",
    "\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# Load the test image\n",
    "im = cv2.imread(test_image_path)\n",
    "\n",
    "outputs = predictor(im)\n",
    "print(\"Bounding boxes:\", outputs[\"instances\"].pred_boxes)\n",
    "print(\"Segmentation masks:\", outputs[\"instances\"].pred_masks)\n",
    "\n",
    "# Visualize the prediction\n",
    "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(\"inria_train\"), scale=1.2)\n",
    "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(out.get_image()[:, :, ::-1])\n",
    "plt.title(\"Inference Result\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data generators for roof type classification ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "train_dir = 'path/to/roof_type/train'\n",
    "val_dir = 'path/to/roof_type/val'\n",
    "\n",
    "# Image data generators with data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "num_classes = len(train_generator.class_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train the roof type classification model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=val_generator.samples // val_generator.batch_size,\n",
    "    epochs=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform obstacle detection using edge detection and color thresholding ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the roof image (segmented roof area)\n",
    "roof_image = cv2.imread('path/to/segmented/roof_image.jpg')\n",
    "\n",
    "# Convert to grayscale\n",
    "gray = cv2.cvtColor(roof_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply Canny edge detection\n",
    "edges = cv2.Canny(gray, threshold1=50, threshold2=150)\n",
    "\n",
    "# Find contours\n",
    "contours, hierarchy = cv2.findContours(edges, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Draw contours on a mask\n",
    "obstacle_mask = np.zeros_like(gray)\n",
    "cv2.drawContours(obstacle_mask, contours, -1, 255, thickness=cv2.FILLED)\n",
    "\n",
    "# Optional: Use color thresholding to refine obstacle detection\n",
    "hsv = cv2.cvtColor(roof_image, cv2.COLOR_BGR2HSV)\n",
    "lower_val = np.array([0, 0, 0])\n",
    "upper_val = np.array([180, 255, 30])  # Adjust values based on obstacle colors\n",
    "color_mask = cv2.inRange(hsv, lower_val, upper_val)\n",
    "\n",
    "# Combine edge and color masks\n",
    "combined_mask = cv2.bitwise_or(obstacle_mask, color_mask)\n",
    "\n",
    "# Visualize the obstacle mask\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(combined_mask, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Obstacle Mask')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the effective roof area excluding obstacles ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume roof_mask is a binary mask of the roof area\n",
    "roof_mask = cv2.imread('path/to/roof_mask.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "roof_area_pixels = cv2.countNonZero(roof_mask)\n",
    "\n",
    "# Obstacle area in pixels\n",
    "obstacle_area_pixels = cv2.countNonZero(combined_mask)\n",
    "\n",
    "# Effective area in pixels\n",
    "effective_area_pixels = roof_area_pixels - obstacle_area_pixels\n",
    "\n",
    "# Spatial resolution in meters per pixel (you need to define this based on your data)\n",
    "resolution = 0.1  # Example: each pixel represents 0.1 meters\n",
    "\n",
    "# Convert to square meters\n",
    "effective_area_m2 = effective_area_pixels * (resolution ** 2)\n",
    "\n",
    "print(f\"Effective roof area available for solar panels: {effective_area_m2:.2f} square meters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect roof faces and calculate orientation angles ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the roof image\n",
    "roof_image = cv2.imread('path/to/roof_image.jpg')\n",
    "gray = cv2.cvtColor(roof_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply edge detection\n",
    "edges = cv2.Canny(gray, threshold1=50, threshold2=150)\n",
    "\n",
    "# Use Hough Line Transform to detect lines\n",
    "lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=100, minLineLength=50, maxLineGap=10)\n",
    "\n",
    "# Create an empty image to draw lines\n",
    "line_image = np.zeros_like(roof_image)\n",
    "\n",
    "# Draw lines on the image\n",
    "if lines is not None:\n",
    "    for line in lines:\n",
    "        x1, y1, x2, y2 = line[0]\n",
    "        cv2.line(line_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "# Overlay lines on the original image\n",
    "overlay_image = cv2.addWeighted(roof_image, 0.8, line_image, 1, 0)\n",
    "\n",
    "# Display the image with detected lines\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(cv2.cvtColor(overlay_image, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.title('Detected Roof Faces')\n",
    "plt.show()\n",
    "\n",
    "# Placeholder for normal vector calculation\n",
    "# In reality, you would need 3D coordinates of the roof faces\n",
    "# For demonstration, let's assume a normal vector\n",
    "normal_vector = np.array([0.5, 0.5, 0.7071])  # Example normal vector\n",
    "\n",
    "# Function to calculate orientation\n",
    "import math\n",
    "\n",
    "def calculate_orientation(normal_vector):\n",
    "    nx, ny, nz = normal_vector\n",
    "    azimuth = math.degrees(math.atan2(ny, nx)) % 360\n",
    "    tilt = math.degrees(math.acos(nz / np.linalg.norm(normal_vector)))\n",
    "    return azimuth, tilt\n",
    "\n",
    "azimuth_angle, tilt_angle = calculate_orientation(normal_vector)\n",
    "\n",
    "print(f\"Azimuth Angle: {azimuth_angle:.2f} degrees\")\n",
    "print(f\"Tilt Angle: {tilt_angle:.2f} degrees\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine if the roof face is suitable for solar panel installation ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 30 <= tilt_angle <= 40:\n",
    "    print(\"The roof face has an ideal tilt angle for solar panels.\")\n",
    "    suitable_area_m2 = effective_area_m2  # Assuming the entire effective area is suitable\n",
    "else:\n",
    "    print(\"The roof face does not have an ideal tilt angle for solar panels.\")\n",
    "    suitable_area_m2 = 0\n",
    "\n",
    "print(f\"Suitable area for solar panels: {suitable_area_m2:.2f} square meters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate potential energy production ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming average solar irradiance and panel efficiency\n",
    "solar_irradiance = 1000  # W/m^2 (average peak sun hours)\n",
    "panel_efficiency = 0.17  # 17% efficiency\n",
    "system_losses = 0.14     # 14% losses\n",
    "\n",
    "# Calculate the estimated energy production\n",
    "daily_energy_kWh = suitable_area_m2 * solar_irradiance * panel_efficiency * (1 - system_losses) * 4 / 1000\n",
    "\n",
    "print(f\"Estimated daily energy production: {daily_energy_kWh:.2f} kWh\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
