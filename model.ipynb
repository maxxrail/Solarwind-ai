{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Necessary Libraries ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ONCE DETECTRON2 IS INSTALLED, PLEASE COPY THE CONTENTS FROM THE SUBFOLDER DETECTRON2 FOLDER TO THE MAIN DETECTRON2 FOLDER!!!\n",
    "\n",
    "# Upgrade pip\n",
    "%pip install --upgrade pip\n",
    "\n",
    "# Install OpenCV for image processing\n",
    "%pip install opencv-python\n",
    "\n",
    "# Install numpy at the desired version\n",
    "%pip install numpy==1.26.1\n",
    "\n",
    "# Install PyTorch and related packages (adjust versions/URLs as needed for your platform)\n",
    "%pip install torch==2.0.0+cu117 torchvision==0.15.1+cu117 torchaudio==2.0.0+cu117 --index-url https://download.pytorch.org/whl/cu117\n",
    "\n",
    "# Install gitpython and cython if not already installed\n",
    "%pip install gitpython cython\n",
    "\n",
    "# Install dependencies, but pin fvcore and pycocotools to compatible versions:\n",
    "%pip install cython\n",
    "\n",
    "# Install fvcore.\n",
    "%pip install fvcore==0.1.5.post20221221\n",
    "\n",
    "# Install pycocotools\n",
    "%pip install pycocotools==2.0.8\n",
    "\n",
    "# ---- Additional installations ----\n",
    "%pip install docutils==0.19\n",
    "%pip install sphinx==7\n",
    "%pip install recommonmark==0.6.0\n",
    "%pip install sphinx_rtd_theme\n",
    "%pip install termcolor\n",
    "%pip install yacs\n",
    "%pip install tabulate\n",
    "%pip install cloudpickle\n",
    "%pip install future\n",
    "# (The following two lines with CPU wheels are for Linux CP37; comment them out if not needed)\n",
    "# %pip install https://download.pytorch.org/whl/cpu/torch-1.8.1%2Bcpu-cp37-cp37m-linux_x86_64.whl\n",
    "# %pip install https://download.pytorch.org/whl/cpu/torchvision-0.9.1%2Bcpu-cp37-cp37m-linux_x86_64.whl\n",
    "%pip install \"omegaconf>=2.1.0.dev24\"\n",
    "%pip install \"hydra-core>=1.1.0.dev5\"\n",
    "%pip install scipy\n",
    "%pip install timm\n",
    "# ---------------------------------\n",
    "\n",
    "# Clone the Detectron2 repository and install it in editable mode\n",
    "import os\n",
    "if not os.path.exists('detectron2'):\n",
    "    !git clone https://github.com/facebookresearch/detectron2.git\n",
    "\n",
    "%cd detectron2\n",
    "%pip install -e .\n",
    "%cd ..\n",
    "\n",
    "# Install TensorFlow and Keras for deep learning models\n",
    "%pip install tensorflow\n",
    "\n",
    "# Install pickleshare to fix the IPython warning\n",
    "%pip install pickleshare\n",
    "\n",
    "# Install additional required libraries\n",
    "%pip install numpy matplotlib pandas scikit-learn tqdm Pillow\n",
    "\n",
    "# Install geospatial libraries\n",
    "%pip install geopandas rasterio shapely\n",
    "\n",
    "# Install any other utility libraries if needed\n",
    "%pip install jsonschema pyyaml\n",
    "\n",
    "# Install IPython kernel to ensure compatibility\n",
    "%pip install ipykernel\n",
    "\n",
    "# After installations, you may need to restart the kernel\n",
    "print(\"All libraries installed. Please restart the kernel to ensure all packages are loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from yacs.config import CfgNode as CN\n",
    "import detectron2.data.transforms as T\n",
    "from detectron2.data import detection_utils as utils\n",
    "from detectron2.engine import DefaultTrainer, DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "print(np.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding Dataset (Do not run unless you need to make dataset bigger, the changes are permanent) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the train folder and its subfolders\n",
    "# train_folder = r\"C:\\Users\\itagl\\Downloads\\AerialImageDataset\\train\"\n",
    "# images_folder = os.path.join(train_folder, \"images\")\n",
    "# gt_folder = os.path.join(train_folder, \"gt\")\n",
    "\n",
    "# # Get a list of all TIF images in the images folder\n",
    "# image_paths = glob.glob(os.path.join(images_folder, \"*.tif\"))\n",
    "# print(f\"Found {len(image_paths)} images in {images_folder}.\")\n",
    "\n",
    "# def rotate_image(img, angle):\n",
    "#     \"\"\"Rotate image by given angle (90, 180, 270 degrees).\"\"\"\n",
    "#     if angle == 90:\n",
    "#         return cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "#     elif angle == 180:\n",
    "#         return cv2.rotate(img, cv2.ROTATE_180)\n",
    "#     elif angle == 270:\n",
    "#         return cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "#     else:\n",
    "#         raise ValueError(\"Unsupported rotation angle: \" + str(angle))\n",
    "\n",
    "# for img_path in image_paths:\n",
    "#     base = os.path.splitext(os.path.basename(img_path))[0]  # e.g. 'austin1'\n",
    "#     ext = os.path.splitext(img_path)[1]  # e.g. '.tif'\n",
    "#     # Assume corresponding mask has the same base name in the gt folder\n",
    "#     mask_path = os.path.join(gt_folder, base + ext)\n",
    "    \n",
    "#     # Read the original image and mask\n",
    "#     img = cv2.imread(img_path)\n",
    "#     mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)  # keep mask as-is\n",
    "#     if img is None or mask is None:\n",
    "#         print(f\"Skipping {base}{ext}: cannot load image or mask.\")\n",
    "#         continue\n",
    "    \n",
    "#     # 1. Rotate 90\n",
    "#     img_r90 = rotate_image(img, 90)\n",
    "#     mask_r90 = rotate_image(mask, 90)\n",
    "    \n",
    "#     # 2. Rotate 180\n",
    "#     img_r180 = rotate_image(img, 180)\n",
    "#     mask_r180 = rotate_image(mask, 180)\n",
    "    \n",
    "#     # 3. Rotate 270\n",
    "#     img_r270 = rotate_image(img, 270)\n",
    "#     mask_r270 = rotate_image(mask, 270)\n",
    "    \n",
    "#     # 4. Mirror (flip horizontally)\n",
    "#     img_m = cv2.flip(img, 1)\n",
    "#     mask_m = cv2.flip(mask, 1)\n",
    "    \n",
    "#     # 5. Rotate 90 then mirror\n",
    "#     img_r90m = cv2.flip(img_r90, 1)\n",
    "#     mask_r90m = cv2.flip(mask_r90, 1)\n",
    "    \n",
    "#     # 6. Rotate 180 then mirror\n",
    "#     img_r180m = cv2.flip(img_r180, 1)\n",
    "#     mask_r180m = cv2.flip(mask_r180, 1)\n",
    "    \n",
    "#     # 7. Rotate 270 then mirror\n",
    "#     img_r270m = cv2.flip(img_r270, 1)\n",
    "#     mask_r270m = cv2.flip(mask_r270, 1)\n",
    "    \n",
    "#     # Save new images in the images folder\n",
    "#     cv2.imwrite(os.path.join(images_folder, f\"{base}r90{ext}\"), img_r90)\n",
    "#     cv2.imwrite(os.path.join(images_folder, f\"{base}r180{ext}\"), img_r180)\n",
    "#     cv2.imwrite(os.path.join(images_folder, f\"{base}r270{ext}\"), img_r270)\n",
    "#     cv2.imwrite(os.path.join(images_folder, f\"{base}m{ext}\"), img_m)\n",
    "#     cv2.imwrite(os.path.join(images_folder, f\"{base}r90m{ext}\"), img_r90m)\n",
    "#     cv2.imwrite(os.path.join(images_folder, f\"{base}r180m{ext}\"), img_r180m)\n",
    "#     cv2.imwrite(os.path.join(images_folder, f\"{base}r270m{ext}\"), img_r270m)\n",
    "    \n",
    "#     # Save new masks in the gt folder\n",
    "#     cv2.imwrite(os.path.join(gt_folder, f\"{base}r90{ext}\"), mask_r90)\n",
    "#     cv2.imwrite(os.path.join(gt_folder, f\"{base}r180{ext}\"), mask_r180)\n",
    "#     cv2.imwrite(os.path.join(gt_folder, f\"{base}r270{ext}\"), mask_r270)\n",
    "#     cv2.imwrite(os.path.join(gt_folder, f\"{base}m{ext}\"), mask_m)\n",
    "#     cv2.imwrite(os.path.join(gt_folder, f\"{base}r90m{ext}\"), mask_r90m)\n",
    "#     cv2.imwrite(os.path.join(gt_folder, f\"{base}r180m{ext}\"), mask_r180m)\n",
    "#     cv2.imwrite(os.path.join(gt_folder, f\"{base}r270m{ext}\"), mask_r270m)\n",
    "    \n",
    "#     print(f\"Created 7 copies for {base}{ext}\")\n",
    "\n",
    "# print(\"Done creating rotated and mirrored copies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement get_inria_dicts() ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inria_dicts_split(img_dir, mask_dir, split=\"train\", train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Returns a list of dataset dictionaries for Detectron2.\n",
    "    Splits the dataset into a training set (80% by default)\n",
    "    and a validation set (20% by default) based on the 'split' parameter.\n",
    "    \n",
    "    Assumes each image in img_dir has a corresponding mask in mask_dir\n",
    "    with the same filename.\n",
    "    \n",
    "    Parameters:\n",
    "      - img_dir: Directory containing the images.\n",
    "      - mask_dir: Directory containing the corresponding masks.\n",
    "      - split: \"train\" or \"val\" to specify the subset.\n",
    "      - train_ratio: Proportion of images for training.\n",
    "    \n",
    "    Returns:\n",
    "      - List of dictionaries with image paths and annotations.\n",
    "    \"\"\"\n",
    "    # Get list of image filenames (filter for common image extensions)\n",
    "    image_files = [f for f in os.listdir(img_dir) if f.lower().endswith((\".jpg\", \".png\", \".jpeg\", \".tif\", \".tiff\"))]\n",
    "    \n",
    "    # Shuffle the list for randomness (set seed if reproducibility is needed)\n",
    "    random.shuffle(image_files)\n",
    "    \n",
    "    # Calculate the split index\n",
    "    split_index = int(len(image_files) * train_ratio)\n",
    "    \n",
    "    # Select files based on split\n",
    "    if split == \"train\":\n",
    "        selected_files = image_files[:split_index]\n",
    "    else:  # \"val\"\n",
    "        selected_files = image_files[split_index:]\n",
    "    \n",
    "    dataset_dicts = []\n",
    "    for idx, image_file in enumerate(selected_files):\n",
    "        record = {}\n",
    "        img_path = os.path.join(img_dir, image_file)\n",
    "        base_name = os.path.splitext(image_file)[0]\n",
    "        mask_path = os.path.join(mask_dir, image_file)  # same filename for mask\n",
    "        \n",
    "        # Read the image to get its dimensions\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        height, width = img.shape[:2]\n",
    "        \n",
    "        record[\"file_name\"] = img_path\n",
    "        record[\"image_id\"] = idx\n",
    "        record[\"height\"] = height\n",
    "        record[\"width\"] = width\n",
    "        \n",
    "        # Read the corresponding mask in grayscale\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if mask is None:\n",
    "            continue\n",
    "        \n",
    "        # Threshold the mask to ensure it's binary\n",
    "        _, thresh = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        objs = []\n",
    "        for cnt in contours:\n",
    "            if cv2.contourArea(cnt) < 10:\n",
    "                continue\n",
    "            poly = cnt.flatten().tolist()\n",
    "            if len(poly) < 6:\n",
    "                continue\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            objs.append({\n",
    "                \"bbox\": [x, y, w, h],\n",
    "                \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                \"segmentation\": [poly],\n",
    "                \"category_id\": 0\n",
    "            })\n",
    "        record[\"annotations\"] = objs\n",
    "        dataset_dicts.append(record)\n",
    "    return dataset_dicts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the dataset for Detectron2 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the dataset with Detectron2\n",
    "# Define the paths to your image and mask folders\n",
    "train_img_dir = r\"C:\\Users\\itagl\\Downloads\\AerialImageDataset\\train\\images\"\n",
    "train_mask_dir = r\"C:\\Users\\itagl\\Downloads\\AerialImageDataset\\train\\gt\"\n",
    "\n",
    "# Register training subset (80% of the image-mask pairs)\n",
    "DatasetCatalog.register(\"inria_train\", lambda: get_inria_dicts_split(train_img_dir, train_mask_dir, split=\"train\"))\n",
    "MetadataCatalog.get(\"inria_train\").set(thing_classes=[\"rooftop\"])\n",
    "\n",
    "# Register validation subset (20% of the image-mask pairs)\n",
    "DatasetCatalog.register(\"inria_val\", lambda: get_inria_dicts_split(train_img_dir, train_mask_dir, split=\"val\"))\n",
    "MetadataCatalog.get(\"inria_val\").set(thing_classes=[\"rooftop\"])\n",
    "\n",
    "print(\"Registered 'inria_train' and 'inria_val' datasets.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize sample data to ensure it's loaded correctly ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a few samples from the dataset\n",
    "dataset_dicts = DatasetCatalog.get(\"inria_train\")\n",
    "num_samples = 3\n",
    "\n",
    "for d in random.sample(dataset_dicts, num_samples):\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    if img is None:\n",
    "        continue\n",
    "    visualizer = Visualizer(img[:, :, ::-1], MetadataCatalog.get(\"inria_train\"), scale=1.0)\n",
    "    out = visualizer.draw_dataset_dict(d)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(out.get_image()[:, :, ::-1])\n",
    "    plt.title(\"Sample Data Visualization\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Mask R-CNN model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration\n",
    "cfg = get_cfg()\n",
    "# Load a Mask R-CNN config from the model zoo\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"))\n",
    "\n",
    "# Specify the training and validation datasets\n",
    "cfg.DATASETS.TRAIN = (\"inria_train\",)\n",
    "cfg.DATASETS.TEST = (\"inria_val\",)  # used for evaluation during training\n",
    "\n",
    "# DataLoader and augmentation settings\n",
    "cfg.DATALOADER.NUM_WORKERS = 12   # adjust based on your CPU capabilities\n",
    "\n",
    "# Load pretrained weights\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\")\n",
    "\n",
    "# Solver settings (adjust iterations and learning rate as needed)\n",
    "cfg.SOLVER.IMS_PER_BATCH = 6    # images per batch\n",
    "cfg.SOLVER.BASE_LR = 0.00025    # learning rate\n",
    "cfg.SOLVER.MAX_ITER = 15000      # number of iterations, adjust based on convergence\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256  # number of proposals per image\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only one class: \"rooftop\"\n",
    "\n",
    "# Optional: Enable built-in random cropping to 4000x4000 if desired\n",
    "cfg.INPUT.CROP.ENABLED = True\n",
    "cfg.INPUT.CROP.TYPE = \"absolute\"\n",
    "cfg.INPUT.CROP.SIZE = [2500, 2500]  # crop size (height, width)\n",
    "\n",
    "# Set the output directory for model checkpoints and logs\n",
    "cfg.OUTPUT_DIR = \"./output_inria\"\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer with the above configuration\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Model on Validation set (You can skip this unless debugging) ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Validate the model on the validation set\n",
    "# evaluator = COCOEvaluator(\"inria_val\", cfg, False, output_dir=cfg.OUTPUT_DIR)\n",
    "# val_loader = build_detection_test_loader(cfg, \"inria_val\")\n",
    "# print(inference_on_dataset(trainer.model, val_loader, evaluator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly select test image ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to you path for test images\n",
    "test_images_dir = r\"C:\\Users\\maxxr\\Downloads\\Pics\\Pics\"\n",
    "\n",
    "# Valid image extensions\n",
    "img_extensions = [\".jpg\", \".jpeg\", \".png\", \".tif\"]\n",
    "all_test_files = [\n",
    "    f for f in os.listdir(test_images_dir)\n",
    "    if os.path.splitext(f)[1].lower() in img_extensions\n",
    "]\n",
    "\n",
    "if not all_test_files:\n",
    "    raise ValueError(\"No valid images found in test_images_dir!\")\n",
    "\n",
    "# Randomly pick one\n",
    "chosen_file = random.choice(all_test_files)\n",
    "test_image_path = os.path.join(test_images_dir, chosen_file)\n",
    "\n",
    "print(\"Randomly chosen test image:\", test_image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform inference using the trained model and visualize ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the configuration to use the final model weights\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.4   # Set the confidence threshold\n",
    "cfg.TEST.DETECTIONS_PER_IMAGE = 1000\n",
    "\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# Load the test image\n",
    "im = cv2.imread(test_image_path)\n",
    "\n",
    "outputs = predictor(im)\n",
    "print(\"Bounding boxes:\", outputs[\"instances\"].pred_boxes)\n",
    "print(\"Segmentation masks:\", outputs[\"instances\"].pred_masks)\n",
    "\n",
    "# Visualize the prediction\n",
    "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(\"inria_train\"), scale=1.2)\n",
    "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(out.get_image()[:, :, ::-1])\n",
    "plt.title(\"Inference Result\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving mask as an image ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = predictor(im)\n",
    "# Access the instance masks (one per detected instance)\n",
    "instance_masks = outputs[\"instances\"].pred_masks  # shape: (N, H, W)\n",
    "\n",
    "# Move masks to CPU and convert to numpy\n",
    "masks = instance_masks.to(\"cpu\").numpy()  # shape: (N, H, W)\n",
    "\n",
    "# Create an empty mask (initialized to zeros - black)\n",
    "combined_mask = np.zeros((masks.shape[1], masks.shape[2]), dtype=np.uint8)\n",
    "\n",
    "# Combine all masks: set pixel to 255 (white) if it belongs to any instance.\n",
    "for mask in masks:\n",
    "    # Ensure the mask is binary (e.g., >0.5 as threshold)\n",
    "    combined_mask[mask > 0.5] = 255\n",
    "\n",
    "# Optionally, save the combined mask image:\n",
    "cv2.imwrite(\"predicted_mask.png\", combined_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform obstacle detection using edge detection and color thresholding ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the image and segmentation mask\n",
    "image_path = test_image_path\n",
    "mask_path = \"predicted_mask.png\"\n",
    "\n",
    "# Physical dimensions of the picture in meters\n",
    "physical_width_m = 986.58\n",
    "physical_height_m = 708.28\n",
    "\n",
    "if os.path.exists(image_path) and os.path.exists(mask_path):\n",
    "    print(\"Files exist\")\n",
    "    # Load the original image and segmentation mask (houses in white, background in black)\n",
    "    image = cv2.imread(image_path)\n",
    "    mask = cv2.imread(mask_path, 0)\n",
    "    \n",
    "    # Create an empty final mask (all black) to hold the processed usable roof areas\n",
    "    final_mask = np.zeros_like(mask)\n",
    "    \n",
    "    # Find all house contours (each white region in the segmentation mask is assumed to be a house)\n",
    "    house_contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if house_contours:\n",
    "        for house_contour in house_contours:\n",
    "            # Calculate bounding box for the current house\n",
    "            x, y, w, h = cv2.boundingRect(house_contour)\n",
    "            \n",
    "            # Zoom into the roof and corresponding mask area using the original mask\n",
    "            largest_roof = image[y:y+h, x:x+w]\n",
    "            roof_mask = mask[y:y+h, x:x+w]\n",
    "            \n",
    "            # Apply the mask to the roof area to isolate it\n",
    "            masked_roof = cv2.bitwise_and(largest_roof, largest_roof, mask=roof_mask)\n",
    "            \n",
    "            # Convert masked roof to grayscale and then to binary\n",
    "            gray_masked_roof = cv2.cvtColor(masked_roof, cv2.COLOR_BGR2GRAY)\n",
    "            blurred_roof = cv2.GaussianBlur(gray_masked_roof, (5, 5), 0)\n",
    "            _, binary_mask = cv2.threshold(blurred_roof, 50, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "            \n",
    "            # Find contours within the masked area from the binary mask\n",
    "            contours_in_roof, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            # Create an image to draw the filled contours (obstacles)\n",
    "            filled_contours_img = np.zeros_like(masked_roof)\n",
    "            \n",
    "            # Define area thresholds to filter out false detections:\n",
    "            min_obstacle_area = 50\n",
    "            max_obstacle_area = (w * h) * 0.3  # Adjust this fraction based on your data\n",
    "            \n",
    "            filtered_contours = []\n",
    "            for cnt in contours_in_roof:\n",
    "                area = cv2.contourArea(cnt)\n",
    "                if min_obstacle_area < area < max_obstacle_area:\n",
    "                    filtered_contours.append(cnt)\n",
    "            \n",
    "            # Draw and fill the filtered contours with green (using your exact code)\n",
    "            cv2.drawContours(filled_contours_img, filtered_contours, -1, (0, 255, 0), thickness=cv2.FILLED)\n",
    "            \n",
    "            # Convert the filled contours image to grayscale, then threshold to obtain a binary obstacle mask\n",
    "            filled_contours_gray = cv2.cvtColor(filled_contours_img, cv2.COLOR_BGR2GRAY)\n",
    "            _, obstacle_mask = cv2.threshold(filled_contours_gray, 1, 255, cv2.THRESH_BINARY)\n",
    "            # Invert the obstacle mask so that obstacles become black\n",
    "            obstacle_mask_inv = cv2.bitwise_not(obstacle_mask)\n",
    "            \n",
    "            # Remove obstacles from the original roof mask (usable roof area remains white)\n",
    "            processed_roof_mask = cv2.bitwise_and(roof_mask, obstacle_mask_inv)\n",
    "            \n",
    "            # Merge the processed roof mask into the final mask at the proper location\n",
    "            final_mask[y:y+h, x:x+w] = cv2.bitwise_or(final_mask[y:y+h, x:x+w], processed_roof_mask)\n",
    "        \n",
    "        # Count white pixels in both the original and processed masks\n",
    "        original_white_pixels = cv2.countNonZero(mask)\n",
    "        processed_white_pixels = cv2.countNonZero(final_mask)\n",
    "        print(\"White pixels in original segmentation mask:\", original_white_pixels)\n",
    "        print(\"White pixels in processed mask (obstacles removed):\", processed_white_pixels)\n",
    "        \n",
    "        # Get the pixel dimensions of the mask\n",
    "        mask_height, mask_width = final_mask.shape  # height and width in pixels\n",
    "        \n",
    "        # Calculate the area each pixel represents.\n",
    "        # Each pixel's physical width: physical_width_m / mask_width\n",
    "        # Each pixel's physical height: physical_height_m / mask_height\n",
    "        area_per_pixel = (physical_width_m / mask_width) * (physical_height_m / mask_height)\n",
    "        \n",
    "        # Total white area (usable roof area) in square meters and square kilometers:\n",
    "        white_area_sqm = processed_white_pixels * area_per_pixel\n",
    "        white_area_sqkm = white_area_sqm / 1e6\n",
    "        \n",
    "        print(\"White area in square meters: {:.2f} m²\".format(white_area_sqm))\n",
    "        print(\"White area in square kilometers: {:.3f} km²\".format(white_area_sqkm))\n",
    "        \n",
    "        # Display the original segmentation mask and the final processed mask side by side\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(mask, cmap='gray')\n",
    "        plt.title(\"Original Segmentation Mask\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(final_mask, cmap='gray')\n",
    "        plt.title(\"Processed Mask (Obstacles Removed)\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No houses detected in the segmentation mask.\")\n",
    "else:\n",
    "    print(\"Files do not exist, check the paths\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the effective roof area excluding obstacles ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume roof_mask is a binary mask of the roof area\n",
    "roof_mask = cv2.imread('/Users/ericdayan/Documents/Documents/University/Capstone/Object Detection.coco/train/img2736_png.rf.58fc1effe9f0eaa92764b325baf93979.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "roof_area_pixels = cv2.countNonZero(roof_mask)\n",
    "\n",
    "# Obstacle area in pixels\n",
    "obstacle_area_pixels = cv2.countNonZero(combined_mask)\n",
    "\n",
    "# Effective area in pixels\n",
    "effective_area_pixels = roof_area_pixels - obstacle_area_pixels\n",
    "\n",
    "# Spatial resolution in meters per pixel (you need to define this based on your data)\n",
    "resolution = 0.2  # Example: each pixel represents 0.1 meters\n",
    "\n",
    "# Convert to square meters\n",
    "effective_area_m2 = effective_area_pixels * (resolution ** 2)\n",
    "\n",
    "print(f\"Effective roof area available for solar panels: {effective_area_m2:.2f} square meters\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
